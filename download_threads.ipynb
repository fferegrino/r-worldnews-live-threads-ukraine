{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import praw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb312813",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "Ever since Russia's \"special military operation\" in Ukraine started, I have been doomscrolling the comments in the [r/worldnews subreddit](https://www.reddit.com/r/worldnews) live threads. I saw with amusement how the frequency of comments increased with each major event but also noticed how each day there were fewer and fewer comments showing a sustained decrease of interest (at least when measured by Reddit comments) on the topic of the invasion.\n",
    "\n",
    "This prompted me to find all the live threads in an attempt to figure out whether my feeling was true or not. The following two posts are a result of this curiosity; in the first one (the one you are reading now) I'll show you how I created the dataset, whereas in the second one, you will find how to use the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988d0db0",
   "metadata": {},
   "source": [
    "## The Reddit API\n",
    "\n",
    "There are a couple of ways to download data from the internet web scraping or APIs (when available): web scraping is my favourite, but at the same time, the most time consuming and fragile to maintain since any change to the layout makes your scraping go wild.\n",
    "\n",
    "Luckily for us, Reddit offers an API one can use to consume data from the site.\n",
    "\n",
    "As with most major websites APIs, to start using this api, one needs to [register an application](https://www.reddit.com/prefs/apps/) - my recommendation is that you create an entirely different Reddit account since you will also have to use the password of said account to authenticate.\n",
    "\n",
    "When your app has been created, make a note of the following values as we will use them too:\n",
    "\n",
    "![Reddit secrets to keep track of](https://ik.imagekit.io/thatcsharpguy/posts/worldnews/created.png?ik-sdk-version=javascript-1.4.3&updatedAt=1651313287355)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b441868",
   "metadata": {},
   "source": [
    "### PRAW to use the Reddit API\n",
    "\n",
    "To consume the API via Python, we will be using the PRAW package. Installable using Python with `pip install praw`.\n",
    "\n",
    "Once we have got our client id and secret we can move on to create a `praw.Reddit` instance passing the information we just got from Reddit; to avoid hardcoding our password and secrets let's use environment variables to set these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f995786",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ[\"CLIENT_ID\"],\n",
    "    client_secret=os.environ[\"CLIENT_SECRET\"],\n",
    "    password=os.environ[\"PASSWORD\"],\n",
    "    user_agent=\"Live Thread Scraper by UkraineNewsBot\",\n",
    "    username=\"UkraineNewsBot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65181b92",
   "metadata": {},
   "source": [
    "### Hashing function\n",
    "\n",
    "We will use a function that takes a string and messes with it in a deterministic manner, this is to \"mask\" some values that I do not think should be made public, or at least, not so easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8dd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_string(content):\n",
    "    return hashlib.md5(content.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46848b5a",
   "metadata": {},
   "source": [
    "## Finding all the threads\n",
    "\n",
    "We need to find all the live threads related to the invasion, as such I will limit my search to begin from the 1st of February 2022 (there were no threads previous to February) and end one day prior to running the search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b59fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_point = datetime(2022, 2, 1)\n",
    "today = datetime.utcnow().replace(hour=0, minute=0, second=0, microsecond=0) - timedelta(hours=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5559b18",
   "metadata": {},
   "source": [
    "Next I define a list of *r/worldnews* moderators, since they are the only ones who are able to create live threads. The list of mods can be obtained using the API itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt:off\n",
    "mods = [\n",
    "    \"qgyh2\", \"maxwellhill\", \"BritishEnglishPolice\", \"anutensil\", \"bennjammin\",\n",
    "    \"DoremusJessup\", \"emmster\", \"green_flash\", \"PraiseBeToScience\", \"WorldNewsMods\",\n",
    "    \"DonTago\", \"istara\", \"Fluttershy_qtest\", \"Surf_Science\", \"imdpathway\",\n",
    "    \"Isentrope\", \"PlanetGuy\", \"alexander1701\", \"wrc-wolf\", \"10ebbor10\",\n",
    "    \"seewolfmdk\", \"mutatron\", \"alfix8\", \"dieyoufool3\", \"MushroomMountain123\",\n",
    "    \"TheEarthquakeGuy\", \"GrumpyFinn\", \"BestFriendWatermelon\", \"NYLaw\", \"hasharin\",\n",
    "    \"tinkthank\", \"DaisyKitty\", \"kwwxis\", \"BlatantConservative\", \"vikinick\",\n",
    "    \"pussgurka\", \"progress18\", \"Morning-Chub\", \"hankhillforprez\", \"Core_Four\",\n",
    "    \"nt337\", \"sunbolts\", \"photonmarchrhopi\", \"PoppinKREAM\", \"Last_Jedi\",\n",
    "    \"ssnistfajen\", \"FreedomsPower\", \"Handicapreader\", \"maybesaydie\", \"_BindersFullOfWomen_\",\n",
    "    \"doc_two_thirty\", \"moombai\", \"abrownn\", \"That_Cupcake\", \"Llim\",\n",
    "    \"slakmehl\", \"MarktpLatz\", \"Mazon_Del\", \"Leerzeichen14\", \"MisterMysterios\",\n",
    "    \"SirT6\", \"Ferelar\", \"Captcha_Imagination\", \"ThaneKyrell\", \"thatnameagain\",\n",
    "    \"loljetfuel\", \"Tidorith\", \"Gunboat_DiplomaC\", \"Petrichordates\", \"Hard_on_Collider\",\n",
    "    \"RedSquirrelFtw\", \"jfoobar\", \"ZippyDan\", \"Yglorba\", \"AftyOfTheUK\",\n",
    "    \"Trips-Over-Tail\", \"Wonckay\", \"Turicus\", \"isnotmad\", \"Iustis\",\n",
    "    \"IsNotACleverMan\", \"Randvek\", \"terminal_mole\", \"grmmrnz\", \"mvea\",\n",
    "    \"Iphotoshopincats\", \"UGMadness\", \"ToadProphet\", \"PapaKnowsDominoes\", \"L_Cranston_Shadow\",\n",
    "    \"allessandro\", \"MSchmahl\", \"indi_n0rd\", \"The_Majestic_\", \"Benocrates\",\n",
    "    \"ThucydidesOfAthens\", \"Emmx2039\", \"valuingvulturefix\", \"Cicero912\", \"whistleridge\",\n",
    "    \"Tetizeraz\", \"Duglitt\", \"ontrack\", \"SecureThruObscure\", \"AdClemson\",\n",
    "    \"jman005\", \"muffpatty\", \"FLAlex111\", \"UrynSM\", \"-doughboy\",\n",
    "    \"AutoModerator\", \"AkaashMaharaj\",\n",
    "]\n",
    "# fmt:on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161fa352",
   "metadata": {},
   "source": [
    "### Iterating over each user\n",
    "\n",
    "The only way I found to find all the threads is to comb all submissions made by mods and then figure out which ones belong to what we care about here. The following fragment of code does that, fetching up to 200 submissions per user and storing them in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = []\n",
    "for username in mods:\n",
    "    user = reddit.redditor(name=username)\n",
    "    for post in user.submissions.new(limit=200):\n",
    "        subs.append(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94d20b",
   "metadata": {},
   "source": [
    "### Iterating over all submisions\n",
    "\n",
    "Once we have all submissions made by mods, we can iterate over them in search of the ones we want. In this case, the ones we want start with either: *\"/r/worldnews live thread\"*, *\"r/worldnews live thread\"* or *\"worldnews live thread\"* and were made between the 1st of february and yesterday:\n",
    "\n",
    "Lastly, to extract all the properties, I am using the `getattr` function in combination with a list of properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec9106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "properties = [\n",
    "    \"id\", \"created_utc\", \"name\", \"num_comments\", \n",
    "    \"permalink\", \"score\", \"title\", \"upvote_ratio\"\n",
    "]\n",
    "# fmt: on\n",
    "\n",
    "\n",
    "def extract_submission_props(post):\n",
    "    post_props = [post.author.name]\n",
    "    post_props.extend([getattr(post, pr) for pr in properties])\n",
    "    return post_props\n",
    "\n",
    "\n",
    "submissions = []\n",
    "for post in subs:\n",
    "    title_low = post.title.lower()\n",
    "    if (\n",
    "        title_low.startswith(\"/r/worldnews live thread\")\n",
    "        or title_low.startswith(\"r/worldnews live thread\")\n",
    "        or title_low.startswith(\"worldnews live thread\")\n",
    "    ) and begin_point.timestamp() < post.created_utc < today.timestamp():\n",
    "        submissions.append(extract_submission_props(post))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be0b4be",
   "metadata": {},
   "source": [
    "### Converting to a DataFrame\n",
    "\n",
    "Once we have all the submissions in a list, we should convert it to a *pandas* DataFrame to make it easy to work with and to save:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14a0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_threads = pd.DataFrame(submissions, columns=[\"author\"] + properties)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce7427",
   "metadata": {},
   "source": [
    "Then we can:\n",
    "\n",
    " - Use `pd.to_datetime` to convert the unix timestamp to an actual date\n",
    " - Hash the author's name with the previously declared `hash_string` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b979589a",
   "metadata": {},
   "outputs": [],
   "source": [
    "live_threads[\"created_at\"] = pd.to_datetime(live_threads[\"created_utc\"], unit=\"s\", origin=\"unix\")\n",
    "live_threads[\"author\"] = live_threads[\"author\"].apply(hash_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b645d9e1",
   "metadata": {},
   "source": [
    "Finally, it is time to save the thread's data with an specified order in the columns, sorted by creation date and without index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d20a0a7",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "live_threads[[\"id\", \"name\", \"author\", \"title\", \"created_utc\", \"created_at\", \"num_comments\", \"score\", \"upvote_ratio\", \"permalink\"]].sort_values(\n",
    "    \"created_utc\", ascending=True\n",
    ").to_csv(\"data/threads.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863de524",
   "metadata": {},
   "source": [
    "## Downloading ALL the comments for a ALL threads\n",
    "\n",
    "The next step is pretty straightforward. We need to iterate over the file we just created and use the PRAW package to download all the comments made to a submission.\n",
    "\n",
    "To begin, let's create a function that takes in a comment and a submission and returns a list of its properties, this function is a bit more complex given that comments differ from one another. Once again, I am using the getattr function to make our lives easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b3608",
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_props = [\n",
    "    \"id\",\n",
    "    \"body\",\n",
    "    \"edited\",\n",
    "    \"created_utc\",\n",
    "    \"link_id\",\n",
    "    \"parent_id\",\n",
    "    \"distinguished\",\n",
    "    \"depth\",\n",
    "    \"ups\",\n",
    "    \"downs\",\n",
    "    \"score\",\n",
    "    \"total_awards_received\",\n",
    "    \"gilded\",\n",
    "]\n",
    "\n",
    "\n",
    "def extract_comment(comment, submission_id):\n",
    "    if comment.author:\n",
    "        cmmt = [hash_string(comment.author.name), submission_id]\n",
    "    else:\n",
    "        cmmt = [None, submission_id]\n",
    "    cmmt.extend([getattr(comment, prop) for prop in comment_props])\n",
    "\n",
    "    if comment.gildings:\n",
    "        gildings = str(comment.gildings)\n",
    "    else:\n",
    "        gildings = None\n",
    "\n",
    "    cmmt.append(gildings)\n",
    "\n",
    "    return cmmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173d4187",
   "metadata": {},
   "source": [
    "We are all set to iterate over the threads downloading all those we do not have yet. [There is a tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html) on the PRAW website itself that details how to download comments to a thread - there is some customisation going on in terms of converting everything to a DataFrame, but the code itself is pretty much self-explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a079580",
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission_id in live_threads[\"id\"]:\n",
    "    file_name = f\"data/comments/comments__{submission_id}.csv\"\n",
    "    if os.path.exists(file_name):\n",
    "        continue\n",
    "\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "    submission.comments.replace_more(limit=1)\n",
    "\n",
    "    comments = []\n",
    "    for comment in submission.comments.list():\n",
    "        comments.append(extract_comment(comment, submission_id))\n",
    "\n",
    "    frame = pd.DataFrame(comments, columns=[\"author\", \"submission_id\"] + comment_props + [\"gildings\"])\n",
    "    frame.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c771da",
   "metadata": {},
   "source": [
    "## Automating everything through GitHub\n",
    "\n",
    "New threads are created every day, which means that if we want to keep our dataset updated, we must run this script every day as well. If you keep your code in GitHub, it sounds like the perfect candidate for automation with GitHub Actions.\n",
    "\n",
    "First off, we will need to save our environment variables with secrets (*CLIENT_ID*, *CLIENT_SECRET*, *PASSWORD*) as repository secrets. To do this, go to *Settings ➡ Secrets (Actions) ➡ New repository secret*:\n",
    "\n",
    "![GitHub secrets](https://ik.imagekit.io/thatcsharpguy/posts/worldnews/secrets-gh.png?ik-sdk-version=javascript-1.4.3&updatedAt=1651614686032)\n",
    "\n",
    "Once all three secrets are available, create a *.yml* file in the *.github/workflows* folder with the following content:\n",
    "\n",
    "```yaml\n",
    "name: Download dataset\n",
    "\n",
    "on:\n",
    "  schedule:\n",
    "  - cron: \"0 10 * * *\"\n",
    "\n",
    "jobs:\n",
    "  process:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - name: Checkout\n",
    "      uses: actions/checkout@v2\n",
    "    - name: Set up Python 3.8\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: \"3.8\"\n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install pipenv\n",
    "        pipenv install --system --dev\n",
    "    - name: Download dataset from Reddit\n",
    "      env:\n",
    "        CLIENT_ID: ${{ secrets.CLIENT_ID }}\n",
    "        CLIENT_SECRET: ${{ secrets.CLIENT_SECRET }}\n",
    "        PASSWORD: ${{ secrets.PASSWORD }}\n",
    "      run: python download_threads.py\n",
    "    - name: Commit changes\n",
    "      run: |\n",
    "        git config --global user.email \"antonio.feregrino@gmail.com\"\n",
    "        git config --global user.name \"Antonio Feregrino\"\n",
    "        git add data/\n",
    "        git diff --quiet && git diff --staged --quiet || git commit -m \"Updated: `date +'%Y-%m-%d %H:%M'`\"\n",
    "        git push\n",
    "```\n",
    "\n",
    "In short, every day at 10 AM:\n",
    "\n",
    " 1. It checkouts the code\n",
    " 2. Sets up Python 3.8\n",
    " 3. Installs the dependencies, in this case I was using pipenv to handle them locally - you could use something entirely different\n",
    " 4. Executes all the previous Python code that downloads the threads and their comments\n",
    " 5. Commits all the changes to the repository, saving our *csv* files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913e61f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "And that is it, now we have downloaded all the relevant threads, and we are ready to use them.\n",
    "\n",
    "In this post, we had a look into how to create a dataset using Reddit data, and in the next one, I'll show you how to use this dataset to create something interesting; I hope you learned something new or at least that you liked it. As always, [code is available here](https://github.com/fferegrino/r-worldnews-live-threads-ukraine/blob/main/download_threads.ipynb), and I am open to answering any question on [Twitter at @io_exception](https://twitter.com/io_exception)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
